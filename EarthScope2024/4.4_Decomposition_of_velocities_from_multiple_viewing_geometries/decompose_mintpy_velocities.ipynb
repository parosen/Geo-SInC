{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extended-priority",
   "metadata": {},
   "source": [
    "# Decompose velocities from multiple lines of sight\n",
    "\n",
    "One of the benefits of having datasets from different viewing geometries (e.g. ascending and descending tracks) is that it is possible to decompose the velocities in areas of overlap into their vertical and horizontal components. Let's try it here! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-board",
   "metadata": {},
   "source": [
    "# 1. Dependencies and functions\n",
    "\n",
    "The dependencies are the same as those that MintPy needs $-$ so make sure they are installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with some dependencies \n",
    "# (needless to say if they are not installed, jupyter will be sad)\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal, osr\n",
    "from mintpy.utils import readfile, utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is little function to find the nearest neighbor to a value in an array\n",
    "# (minimal alteration from stack overflow)\n",
    "def closest(lst, K):  \n",
    "     lst = np.asarray(lst)\n",
    "     idx = (np.abs(lst - K)).argmin()\n",
    "     return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-organ",
   "metadata": {},
   "source": [
    "## 2. Load in and crop the data\n",
    "\n",
    "The first step is to read in the data. Here we are using data from MintPy, although this ought to work for other datasets too. Each MintPy file contains a mixture of data layers ('subdatasets') and metadata information ('attributes') $-$ and we want both, ideally. \n",
    "\n",
    "This is a bit fiddly... we need to first read the attributes for each track in turn, to figure out the file dimensions and coordinates. Then we want load in multiple data layers from multiple files, and crop them to the same dimensions. Reading the attributes is easiest using one of the MintPy utilities, and we'll use Gdal for the actual data loading. The data files are HDF5 format, which is versatile, but potentially very complicated! \n",
    "\n",
    "The file 'velocity.h5' for each track contains two layers $-$ the velocity data, and the $1\\sigma$ uncertainty in the velocity. We want them both.\n",
    "\n",
    "The other file, 'geometryGeo.h5' for each track contains four layers. We want two of them $-$ layer 0 (which contains 'azimuth' $-$ pointing direction $-$ information), and layer 2 (which contains incidence information).\n",
    "\n",
    "To add a little more complication, it seems that MintPy is a little sloppy with coordinates, in that data which ought to have the same pixel coordinates, don't, and distances which ought to be spanned by neat round numbers of pixels (by design!) aren't. So I had to bodge it a little. Oh well... it's probably accurate to a pixel or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for MintPy h5 files\n",
    "# some file locations, names and details\n",
    "dscdir='data/d071/MintPy/'\n",
    "ascdir='data/a064/MintPy/'\n",
    "\n",
    "# define your crop area\n",
    "crop_minx=-118.0\n",
    "crop_maxx=-117.6\n",
    "crop_maxy=36.15\n",
    "crop_miny=35.9\n",
    "\n",
    "# MintPy filenames\n",
    "velofile='velocity.h5'\n",
    "geomfile='inputs/geometryGeo.h5'\n",
    "\n",
    "# ------\n",
    "# we begin with the descending data\n",
    "\n",
    "# read in data and metadata for the descending data\n",
    "print('reading data file '+dscdir+velofile)\n",
    "data, atr = readfile.read(dscdir+velofile, datasetName='velocity')\n",
    "\n",
    "# extract lat/lon information from the attributes\n",
    "lat0 = float(atr['Y_FIRST']);  lat_step = float(atr['Y_STEP']);  lat_num = int(atr['LENGTH'])\n",
    "lon0 = float(atr['X_FIRST']);  lon_step = float(atr['X_STEP']);  lon_num = int(atr['WIDTH'])\n",
    "lat1 = lat0 + lat_step * lat_num\n",
    "lon1 = lon0 + lon_step * lon_num\n",
    "lats, lons = np.mgrid[lat0:lat1:lat_num*1j,\n",
    "                      lon0:lon1:lon_num*1j]\n",
    "\n",
    "# find the crop extents\n",
    "mincol = closest(lons[0], crop_minx)\n",
    "maxcol = closest(lons[0], crop_maxx)\n",
    "maxrow = closest(lats[:,0], crop_miny)\n",
    "minrow = closest(lats[:,0], crop_maxy)\n",
    "\n",
    "# optional bodge\n",
    "maxcol+=1\n",
    "maxrow+=1\n",
    "\n",
    "print('  minimum longitude: {0:f}, closest column {1:d}, closest longitude: {2:f}'\n",
    "      .format(crop_minx, mincol, lons[0,mincol]))\n",
    "print('  maximum longitude: {0:f}, closest column {1:d}, closest longitude: {2:f}'\n",
    "      .format(crop_maxx, maxcol, lons[0,maxcol]))\n",
    "print('  minimum latitude: {0:f}, closest row {1:d}, closest latitude: {2:f}'\n",
    "      .format(crop_miny, maxrow, lats[maxrow,0]))\n",
    "print('  maximum latitude: {0:f}, closest row {1:d}, closest latitude: {2:f}'\n",
    "      .format(crop_maxy, minrow, lats[minrow,0]))\n",
    "\n",
    "# read in velocity and error data, and crop them\n",
    "dscveloin=gdal.Open(dscdir+velofile,gdal.GA_ReadOnly)\n",
    "dsctmp=gdal.Open(dscveloin.GetSubDatasets()[0][0]).ReadAsArray()\n",
    "dscvel=dsctmp[minrow:maxrow,mincol:maxcol]\n",
    "dsctmp=gdal.Open(dscveloin.GetSubDatasets()[1][0]).ReadAsArray()\n",
    "dscerr=dsctmp[minrow:maxrow,mincol:maxcol]\n",
    "\n",
    "# read in azimuth and incidence information\n",
    "dscgeomin=gdal.Open(dscdir+geomfile,gdal.GA_ReadOnly)\n",
    "dsctmp=gdal.Open(dscgeomin.GetSubDatasets()[0][0]).ReadAsArray()\n",
    "dscazi=dsctmp[minrow:maxrow,mincol:maxcol]\n",
    "dsctmp=gdal.Open(dscgeomin.GetSubDatasets()[2][0]).ReadAsArray()\n",
    "dscinc=dsctmp[minrow:maxrow,mincol:maxcol]\n",
    "\n",
    "\n",
    "# ------\n",
    "# and now for the ascending data...\n",
    "\n",
    "# read in data and metadata for the descending data\n",
    "print('reading data file '+ascdir+velofile)\n",
    "data, atr = readfile.read(ascdir+velofile, datasetName='velocity')\n",
    "\n",
    "# extract lat/lon information from the attributes\n",
    "lat0 = float(atr['Y_FIRST']);  lat_step = float(atr['Y_STEP']);  lat_num = int(atr['LENGTH'])\n",
    "lon0 = float(atr['X_FIRST']);  lon_step = float(atr['X_STEP']);  lon_num = int(atr['WIDTH'])\n",
    "lat1 = lat0 + lat_step * lat_num\n",
    "lon1 = lon0 + lon_step * lon_num\n",
    "lats, lons = np.mgrid[lat0:lat1:lat_num*1j,\n",
    "                      lon0:lon1:lon_num*1j]\n",
    "\n",
    "# find the crop extents\n",
    "mincol = closest(lons[0], crop_minx)\n",
    "maxcol = closest(lons[0], crop_maxx)\n",
    "maxrow = closest(lats[:,0], crop_miny)\n",
    "minrow = closest(lats[:,0], crop_maxy)\n",
    "\n",
    "# optional bodge\n",
    "maxcol+=1\n",
    "maxrow+=1\n",
    "\n",
    "print('  minimum longitude: {0:f}, closest column {1:d}, closest longitude: {2:f}'\n",
    "      .format(crop_minx, mincol, lons[0,mincol]))\n",
    "print('  maximum longitude: {0:f}, closest column {1:d}, closest longitude: {2:f}'\n",
    "      .format(crop_maxx, maxcol, lons[0,maxcol]))\n",
    "print('  minimum latitude: {0:f}, closest row {1:d}, closest latitude: {2:f}'\n",
    "      .format(crop_miny, maxrow, lats[maxrow,0]))\n",
    "print('  maximum latitude: {0:f}, closest row {1:d}, closest latitude: {2:f}'\n",
    "      .format(crop_maxy, minrow, lats[minrow,0]))\n",
    "\n",
    "# read in velocity and error data, and crop them\n",
    "ascveloin=gdal.Open(ascdir+velofile,gdal.GA_ReadOnly)\n",
    "asctmp=gdal.Open(ascveloin.GetSubDatasets()[0][0]).ReadAsArray()\n",
    "ascvel=asctmp[minrow:maxrow,mincol:maxcol]\n",
    "asctmp=gdal.Open(ascveloin.GetSubDatasets()[1][0]).ReadAsArray()\n",
    "ascerr=asctmp[minrow:maxrow,mincol:maxcol]\n",
    "\n",
    "# read in azimuth and incidence information\n",
    "ascgeomin=gdal.Open(ascdir+geomfile,gdal.GA_ReadOnly)\n",
    "asctmp=gdal.Open(ascgeomin.GetSubDatasets()[0][0]).ReadAsArray()\n",
    "ascazi=asctmp[minrow:maxrow,mincol:maxcol]\n",
    "asctmp=gdal.Open(ascgeomin.GetSubDatasets()[2][0]).ReadAsArray()\n",
    "ascinc=asctmp[minrow:maxrow,mincol:maxcol]\n",
    "\n",
    "# ------\n",
    "# and for one final flourish, we can make some coordinate matrices, masks, all the good stuff \n",
    "\n",
    "mask=((dscvel!=0)&(ascvel!=0))\n",
    "nrows = dscvel[:,0].size\n",
    "ncols = dscvel[0].size\n",
    "xx=np.linspace(crop_minx,crop_maxx,ncols)\n",
    "yy=np.linspace(crop_maxy,crop_miny,nrows)\n",
    "X, Y = np.meshgrid(xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-basin",
   "metadata": {},
   "source": [
    "We can check that the data loaded OK, with a little light plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the cropped data\n",
    "fig1, ax1 = plt.subplots()                       # initiate a matplotlib plot\n",
    "im1 = ax1.imshow(dscvel)                         # plot displacements\n",
    "ax1.set_title('descending LOS velocities (m/yr)')\n",
    "fig1.colorbar(im1)                                # plot a color bar!\n",
    "fig2, ax2 = plt.subplots()                       # initiate a matplotlib plot\n",
    "im2 = ax2.imshow(ascvel)                         # plot displacements \n",
    "ax2.set_title('ascending LOS velocities (m/yr)')\n",
    "fig2.colorbar(im2)                                # plot a color bar!\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-league",
   "metadata": {},
   "source": [
    "## 3. Apply a reference point\n",
    "\n",
    "We want to give our datasets a common reference point (in case you didn't already), so that zero velocity is consistent. I recommend you pick a stable location, i.e. not a place where there is a lot of deformation, such as a bedrock outcrop if possible (look in Google Earth). Even better $-$ if there are any nearby, pick a stable location with a continuous GNSS station!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long and lat of a stable reference point\n",
    "ref_x=-117.8322\n",
    "ref_y=35.9567 \n",
    "\n",
    "# and find it in your coordinate system\n",
    "refcol=closest(xx,ref_x)\n",
    "refrow=closest(yy,ref_y)\n",
    "\n",
    "# shift and mask your data\n",
    "dscvel=(dscvel-dscvel[refrow,refcol])*mask\n",
    "ascvel=(ascvel-ascvel[refrow,refcol])*mask\n",
    "\n",
    "# wasn't that easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-baseline",
   "metadata": {},
   "source": [
    "## 4. Loop through the images and invert for vertical and horizontal!\n",
    "\n",
    "This is the meaty part of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what sign convention are you using?\n",
    "sign_convention=-1   # 1 for range change, -1 for ground displacement (MintPy default)\n",
    "\n",
    "# report the sign convention\n",
    "if sign_convention==1:\n",
    "    print('range change sign convention')\n",
    "elif sign_convention==-1:\n",
    "    print('ground displacement sign convention')\n",
    "else:\n",
    "    print('you may have messed up this sign convention thing, to be honest')\n",
    "\n",
    "\n",
    "# make a place to put your outputs\n",
    "hvel=np.zeros((nrows,ncols))\n",
    "herr=np.zeros((nrows,ncols))\n",
    "vvel=np.zeros((nrows,ncols))\n",
    "verr=np.zeros((nrows,ncols))\n",
    "\n",
    "# loop through rows and columns of the data!\n",
    "\n",
    "for i in range(nrows):\n",
    "    \n",
    "    for j in range(ncols):\n",
    "        \n",
    "        # if there isn't any data there\n",
    "        if dscvel[i,j]==0 and ascvel[i,j]==0:     # null pixel\n",
    "            \n",
    "            # and it's not the reference point\n",
    "            if i!=refrow and j!=refcol:\n",
    "                hvel[i,j]=math.nan;      # blank out the output!\n",
    "                herr[i,j]=math.nan;\n",
    "                vvel[i,j]=math.nan;\n",
    "                verr[i,j]=math.nan;\n",
    "            \n",
    "        else:                                     # we have a live one!\n",
    "            \n",
    "            # calculate the unit LOS vectors (satellite pointing to ground target)\n",
    "            # (this all assumes angles using the ISCE convention, in degrees)\n",
    "            \n",
    "            dsc_los=np.array([np.sin(np.radians(dscazi[i,j]))*np.sin(np.radians(dscinc[i,j])),\n",
    "                  -np.cos(np.radians(dscazi[i,j]))*np.sin(np.radians(dscinc[i,j])),\n",
    "                  -np.cos(np.radians(dscinc[i,j]))])*sign_convention\n",
    "\n",
    "            asc_los=np.array([np.sin(np.radians(ascazi[i,j]))*np.sin(np.radians(ascinc[i,j])),\n",
    "                  -np.cos(np.radians(ascazi[i,j]))*np.sin(np.radians(ascinc[i,j])),\n",
    "                  -np.cos(np.radians(ascinc[i,j]))])*sign_convention\n",
    "\n",
    "            # take east and vertical components of LOS and make the design matrix from them\n",
    "            A=np.array([[dsc_los[0], dsc_los[2]],\n",
    "                        [asc_los[0], asc_los[2]]])\n",
    "            \n",
    "            # take the errors from the data and make inverse variance weights from them\n",
    "            W=np.array([[1/dscerr[i,j]**2, 0],\n",
    "                        [0, 1/ascerr[i,j]**2]])\n",
    "            \n",
    "            # and finally, put the data into a column vector\n",
    "            d=np.array([dscvel[i,j], ascvel[i,j]]).T\n",
    "            \n",
    "            # and invert!\n",
    "            ATAinv=np.linalg.inv(np.matmul(np.matmul(A.T,W),A))\n",
    "            ATd=np.matmul(np.matmul(A.T,W),d)\n",
    "            m=np.matmul(ATAinv,ATd)\n",
    "            \n",
    "            # and then extract your answers\n",
    "            hvel[i,j]=m[0]                   # horizontal velocity\n",
    "            herr[i,j]=np.sqrt(ATAinv[0,0])   # 1-sigma uncertainty in horizontal velocity\n",
    "            vvel[i,j]=m[1]                   # vertical velocity\n",
    "            verr[i,j]=np.sqrt(ATAinv[1,1])   # ...and the 1-sigma uncertainty in that\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the output\n",
    "fig1, ax1 = plt.subplots()                       # initiate a matplotlib plot\n",
    "im1 = ax1.imshow(hvel)                         # plot displacements\n",
    "ax1.set_title('E-W velocity (m/yr)')\n",
    "fig1.colorbar(im1)                                # plot a color bar!\n",
    "fig2, ax2 = plt.subplots()                       # initiate a matplotlib plot\n",
    "im2 = ax2.imshow(vvel)                         # plot displacements \n",
    "ax2.set_title('vertical velocity (m/yr)')\n",
    "fig2.colorbar(im2)                                # plot a color bar!\n",
    "\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some file names\n",
    "outvert='coso_vertical'\n",
    "outhorz='coso_e-w'\n",
    "\n",
    "# try to write out the vertical data in GeoTiff format\n",
    "driver = gdal.GetDriverByName(\"GTiff\")\n",
    "outgrd = driver.Create(outvert+'.tiff', ncols, nrows, 1, gdal.GDT_Float32)\n",
    "outgrd.SetGeoTransform((crop_minx, lon_step, 0, crop_maxy, 0, lat_step))\n",
    "outgrdSRS = osr.SpatialReference()\n",
    "outgrdSRS.ImportFromEPSG(4326)\n",
    "outgrd.SetProjection(outgrdSRS.ExportToWkt())\n",
    "outband = outgrd.GetRasterBand(1)\n",
    "outband.WriteArray(vvel,0,0)\n",
    "outband.FlushCache()         # need this to actually write anything!\n",
    "outband.SetNoDataValue(0)\n",
    "del outgrd, outband, outgrdSRS\n",
    "\n",
    "# and again for the horizontal data\n",
    "driver = gdal.GetDriverByName(\"GTiff\")\n",
    "outgrd = driver.Create(outhorz+'.tiff', ncols, nrows, 1, gdal.GDT_Float32)\n",
    "outgrd.SetGeoTransform((crop_minx, lon_step, 0, crop_maxy, 0, lat_step))\n",
    "outgrdSRS = osr.SpatialReference()\n",
    "outgrdSRS.ImportFromEPSG(4326)\n",
    "outgrd.SetProjection(outgrdSRS.ExportToWkt())\n",
    "outband = outgrd.GetRasterBand(1)\n",
    "outband.WriteArray(hvel,0,0)\n",
    "outband.FlushCache()\n",
    "outband.SetNoDataValue(0)\n",
    "del outgrd, outband, outgrdSRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc81029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthscope_insar [conda env:.local-earthscope_insar]",
   "language": "python",
   "name": "conda-env-.local-earthscope_insar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
