{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edf8e3f-f4df-46f9-b2eb-12ac4b76ba14",
   "metadata": {},
   "source": [
    "# Constructing dense offset maps from Sentinel-1 A/B SLC Bursts using topsApp and autoRIFT\n",
    "\n",
    "**Authors**: Bryan Riel, Forrest Williams, Franz Meyer\n",
    "\n",
    "In this notebook, we will explore how to generate dense offset maps (feature tracking) from co-registered Sentinel-1 Burst SLCs within the `topsApp` processing chain. As before, after obtaining the appropriate Burst SLC data, we will prepare a DEM for our study area, and download the necessary orbit and aux files from ASF/ESA (see previous tutorials on processing in TOPS mode with `topsApp.py`).\n",
    "\n",
    "## Comparison of dense offsets vs. InSAR phase\n",
    "\n",
    "Dense offsets maps are useful in situations where ground or surface displacement is large between two acquisition dates. In these cases, reference and secondary SLCs can not be precisely aligned, and we would get complete decoherence in the interferometric phase. Such situations are common in glaciology, certain geomorphology applications (e.g., landslides), and for very large earthquakes. Additionally, dense offsets automatically give us displacement in two directions (range and azimuth), whereas InSAR will only provide phase changes in the range direction. Thus, the range offsets measure motion in the radar line-of-sight direction, whereas the azimuth offsets measure horizontal motion in the radar flight direciton (along-track). \n",
    "\n",
    "Of course, the main disadvantage of offset maps is the much higher level of noise in the cross-correlation measurements as compared to InSAR phase. For example, the figure below compares InSAR phase and range pixel offsets for the 2021 M7.2 Haiti earthquake (obtained from Eric Fielding Twitter https://twitter.com/EricFielding/status/1428629519947034624):\n",
    "\n",
    "<img src=\"support_files/HaitiOffsets.png\" width=900/>\n",
    "\n",
    "Note, this comparison is only valid if the InSAR phase is not decorrelated. In general, offsets will provide a more robust, albeit noisier, estimate of surface displacment. Another disadvantage is that dense offset map generation takes considerably more compute power as compared to interferogram formation, but ISCE does provide GPU implementations of dense offsets to significantly reduce computation time.\n",
    "\n",
    "## Study area and setup\n",
    "\n",
    "The Malaspina Glacier is a prominent example of a piedmont glacier, distinguished by its expansive flow dynamics and unique geometry. Spanning approximately 3,900 square kilometers (1,500 square miles), it forms from the convergence of multiple valley glaciers descending from the Saint Elias Mountains. Unlike typical valley glaciers constrained by narrow paths, the Malaspina spreads out widely over a coastal plain as it approaches the Gulf of Alaska, creating a broad, lobe-like shape. This spreading is facilitated by its immense ice volume and relatively gentle slope, allowing the glacier to flow laterally as well as forward. The glacier's flow dynamics are complex, involving interactions between ice movement, sediment transport, and surface melting. The presence of moraines and ice-cored hills indicates significant sediment deposition and ice deformation. These features provide valuable insights into the glacier's history and the processes shaping its evolution.\n",
    "\n",
    "<img style=\"padding: 10px 10px 10px 10px\" src=\"Figs/Malaspina-Hubbard.jpg\" width=\"750\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81928c85",
   "metadata": {},
   "source": [
    "## `topsApp` and Burst SLC products\n",
    "\n",
    "In this notebook, we will utilize the newly-released Burst SLC products by ASF.\n",
    "\n",
    "https://asf.alaska.edu/datasets/daac/sentinel-1-bursts/\n",
    "\n",
    "\"ASF offers derived burst-based products for Sentinel-1 single look complex (SLC) data collected via the Interferometric Wide (IW) mode and Extra-Wide Swath (EW) modes*. These products are particularly useful for scientists interested in obtaining data for small geographic areas and workflows requiring precise alignment of co-geolocated SLC data through time (i.e., time series analyses)...\"\n",
    "\n",
    "\"...While data from many bursts are packaged together in the standard SLC products, SLC data from a single burst can be utilized independently. To enable single-burst applications, ASF has developed an extraction service that allows users to download single-burst IW and EW SLC products. The burst SLC data are available as a GeoTIFF and supplemental metadata files from the parent SLC product are available as a single XML file.\"\n",
    "\n",
    "<img src=\"Figs/burst_frames.png\" width=700/>\n",
    "\n",
    "Let's first import necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4f843-879c-4b09-aabd-e9f7cb2795cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import datetime\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "from osgeo import gdal, osr\n",
    "gdal.UseExceptions()\n",
    "import scipy.ndimage as ndimage\n",
    "import isce\n",
    "\n",
    "# Some utility functions for plotting and loading data\n",
    "%aimport utilities\n",
    "utils = utilities\n",
    "\n",
    "# autoRIFT and Geogrid\n",
    "%aimport testGeogrid_ISCE\n",
    "%aimport testautoRIFT_ISCE\n",
    "loadMetadata = testGeogrid_ISCE.loadMetadata\n",
    "runGeogrid = testGeogrid_ISCE.runGeogrid\n",
    "generateAutoriftProduct = testautoRIFT_ISCE.generateAutoriftProduct\n",
    "    \n",
    "DEFAULT_PARAMETER_FILE = '/vsicurl/http://its-live-data.s3.amazonaws.com/' \\\n",
    "                         'autorift_parameters/v001/autorift_landice_0120m.shp'\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "plt.rc('font', size=13)\n",
    "parent_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bc55d-586b-4fff-a0a1-bf0288f48cdd",
   "metadata": {},
   "source": [
    "The burst SLCs can be downloaded from Vertex or with the ASF API. The `burst2safe` package (https://github.com/forrestfwilliams/burst2safe) provides a convenient way to automatically download burst SLCs and convert them to an equivalent ESA SAFE file that can be processed with ISCE just like a normal Sentinel-1 SLC. In this lab, we have already run the `burst2stack` command for the Malaspina glacier, which allows us to automatically query bursts for a given region, relative orbit, and date range. An example call to `burst2stack` looks like:\n",
    "\n",
    "```\n",
    "> burst2stack --rel-orbit 50 --start-date 2020-05-09 --end-date 2020-05-25 --pols VV --min-bursts 2 --output-dir safe --extent -140.8291 59.9193 -140.1692 60.3013\n",
    "Creating SAFEs for 2 time periods...\n",
    "\n",
    "Using burst group search...\n",
    "Found 3 burst(s).\n",
    "Check burst group validity...\n",
    "Downloading data...\n",
    "Download complete.\n",
    "Creating SAFE...\n",
    "SAFE created!\n",
    "\n",
    "Using burst group search...\n",
    "Found 3 burst(s).\n",
    "Check burst group validity...\n",
    "Downloading data...\n",
    "Download complete.\n",
    "Creating SAFE...\n",
    "SAFE created!\n",
    "```\n",
    "\n",
    "In our case, we have pre-downloaded three burst for two different dates. To unpack the burst data, as well as orbits, DEMS, and auxiliary data, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74b6aa-2830-42c6-8880-8a16b373cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --region=us-west-2 --no-sign-request s3 cp s3://asf-jupyter-data-west/malaspina_bursts.tar.gz malaspina_bursts.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcecfe-256c-4938-82d5-3b1aa62192b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf malaspina_bursts.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986e160-a4bc-4413-bc0b-ad4f7fe4af11",
   "metadata": {},
   "source": [
    "After extraction, we can see that the burst data are in the `safe` directory. Additionally, we have several NASADEM DEMs, as well as orbit and auxiliary directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce288983",
   "metadata": {},
   "source": [
    "### Configuring topsApp\n",
    "\n",
    "Our `topsApp` configuration in the file `topsApp.xml` contains the following:\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<topsApp>\n",
    "  <component name=\"topsinsar\">\n",
    "    <property name=\"Sensor name\">SENTINEL1</property>\n",
    "\n",
    "    <component name=\"reference\">\n",
    "      <property name=\"orbit directory\">orbits</property>\n",
    "      <property name=\"auxiliary data directory\">aux</property>\n",
    "      <property name=\"output directory\">reference</property>\n",
    "      <property name=\"safe\">safe/S1A_IW_SLC__1SSV_20200510T025515_20200510T025521_032497_03C36B_A924.SAFE</property>\n",
    "    </component>\n",
    "\n",
    "    <component name=\"secondary\">\n",
    "      <property name=\"orbit directory\">orbits</property>\n",
    "      <property name=\"auxiliary data directory\">aux</property>\n",
    "      <property name=\"output directory\">secondary</property>\n",
    "      <property name=\"safe\">safe/S1A_IW_SLC__1SSV_20200522T025516_20200522T025522_032672_03C8B8_64C9.SAFE</property>\n",
    "    </component> \n",
    "\n",
    "    <property name=\"range looks\">7</property>\n",
    "    <property name=\"azimuth looks\">3</property>\n",
    "    <property name=\"demFilename\">nasadem_wgs84.dem</property>\n",
    "    <property name=\"region of interest\">[59.919, 60.301, -140.829, -140.169]</property>\n",
    "\n",
    "    <!-- For dense offsets, we can skip many of the refinement steps used for InSAR -->\n",
    "    <property name=\"do interferogram\">False</property>\n",
    "    <property name=\"do ESD\">False</property>\n",
    "    <property name=\"do unwrap\">False</property>\n",
    "\n",
    "    <!-- Parameters for dense offsets -->\n",
    "    <property name=\"do denseoffsets\">True</property>\n",
    "    <property name=\"Ampcor window width\">128</property>\n",
    "    <property name=\"Ampcor window height\">64</property>\n",
    "    <property name=\"Ampcor search window width\">20</property>\n",
    "    <property name=\"Ampcor search window height\">10</property>\n",
    "    <property name=\"Ampcor skip width\">64</property>\n",
    "    <property name=\"Ampcor skip height\">32</property>\n",
    "\n",
    "  </component>\n",
    "</topsApp>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cd080-35c1-40bc-a313-fc05ec3a348d",
   "metadata": {},
   "source": [
    "In short, we have configured `topsApp` to co-register the burst SLCs while skipping processing steps necessary for interferogram formation (e.g., ESD, ionospheric corrections, unwrapping, etc.). Also pay attention to the final block of parameters for the dense offset computation and compare them to the following illustration. \n",
    "\n",
    "<img src=\"support_files/offset_parameters.png\" width=700>\n",
    "\n",
    "Note that in the Sentinel-1 IW mode, the SLC pixel spacing is roughly 2.3 meters in range and 14.1 meters in azimuth. Therefore, one can design the window widths and heights in order to achieve roughly square pixels in geographical space. We'll explore the impact of these offset parameters in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afee71-21b0-4ff6-bd6c-2c2396c29b72",
   "metadata": {},
   "source": [
    "## Run `topsApp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d54591-0571-4fba-9275-0ab2274f1de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!topsApp.py --start=startup --end=preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a14eb-13cb-496e-b54e-1725fe7a9212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!topsApp.py --start=computeBaselines --end=geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfc23b-478b-42f5-a3d2-72a679e9665c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!topsApp.py --start=denseoffsets --end=geocodeoffsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06700c6c-20fd-493d-b98c-3cd34e14608c",
   "metadata": {},
   "source": [
    "## Overview on dense offsets\n",
    "\n",
    "The basic mathematical operation underpinning dense offset creation is 2D cross-correlation, which is a generalization of cross-correlation/discrete convolution in 1D (Wikipedia: https://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif)\n",
    "\n",
    "<img src=\"support_files/convolution1d_wiki.gif\" width=600>\n",
    "\n",
    "For a given patch of a reference image, we extract a template patch from a secondary image (where the secondary image has been co-registered to the reference image), and we slide the template patch over the reference patch and compute the 2D cross-correlation at each sliding position. This operation creates a 2D \"correlation surface\", and the coordinate of the peak of that surface tells us the nominal displacement (offset) between the two patches. The schematic below illustrates this procedure\n",
    "\n",
    "![Template matching schematic](support_files/ampcor_schematic.png)\n",
    "\n",
    "In the above cartoon, we slid the template over the source image at 9 discrete locations (3x3 search), and the peak correlation is at index (1, 1). By repeating this procedure for a dense array of patches, we can create a dense offset map, e.g.:\n",
    "\n",
    "<img src=\"support_files/ampcor_global.png\" width=300 height=600/>\n",
    "\n",
    "In ISCE and `topsApp.py`, 2D cross-correlation is performed on reference and secondary SLCs. The complex data are converted into magnitude images prior to cross-correlation, and the offset map parameters can be configured in the top-level `topsApp.xml` file. In addition to nominal offsets, ISCE provides estimates of correlation signal-to-noise ratio (SNR) and covariance, where the latter is estimated by computing the curvature of the correlation surface.\n",
    "\n",
    "### 1D example\n",
    "\n",
    "To explore the key parts of template matching, let's look at a 1D example. Here, we generate a synthetic signal, `x`, and create a shifted version, `y`, which will also contain some data noise. The shift includes both integer and fractional shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36d95b-7fb5-493e-98e5-fb9069e073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate, butter, filtfilt\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def apply_fractional_shift(signal, shift):\n",
    "    n = len(signal)\n",
    "    signal_fft = np.fft.fft(signal)\n",
    "    freq = np.fft.fftfreq(n)\n",
    "    shift_fft = np.exp(-2j * np.pi * freq * shift)\n",
    "    shifted_signal = np.real(np.fft.ifft(signal_fft * shift_fft))\n",
    "    return shifted_signal\n",
    "\n",
    "# Generate synthetic signals\n",
    "def generate_signals(shift, noise_level, cutoff=0.05, fs=1.0, order=4, length=1000, pad=50, seed=0):\n",
    "    # Generate white noise with padding\n",
    "    x = np.random.randn(length + 2*pad)\n",
    "    # Low-pass filter and remove padding\n",
    "    x = lowpass_filter(x, cutoff, fs, order)[pad:-pad]\n",
    "    # Apply shift and add noise\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = apply_fractional_shift(x, shift) + noise_level * rng.standard_normal(length)\n",
    "    return x, y\n",
    "\n",
    "# Generate reference and shifted signals\n",
    "true_shift = 5.5\n",
    "x, y = generate_signals(true_shift, 0.1)\n",
    "\n",
    "plt.plot(x, label='Reference')\n",
    "plt.plot(y, label='Shifted')\n",
    "plt.ylabel('Signal amplitude')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b713f-fe8f-49b6-af7a-08e33aa21890",
   "metadata": {},
   "source": [
    "We now use `scipy.signal.correlate` to generate a 1D correlation \"surface\" (signal). The estimated integer shift is the location of the peak of the correlation signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896419c0-809c-4c08-a4e7-b46673cdc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = correlate(y, x, mode='full')\n",
    "peak_index = np.argmax(corr)\n",
    "normalized_corr = corr / corr[peak_index]\n",
    "\n",
    "c_index = np.arange(normalized_corr.size) - 1000\n",
    "plt.plot(c_index, normalized_corr)\n",
    "plt.xlabel('Integer offset')\n",
    "plt.ylabel('Normalized cross-correlation');\n",
    "#plt.xlim(-100, 100) # un-comment to zoom in on correlation peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16352dff-4625-4f45-a368-f362966ccc17",
   "metadata": {},
   "source": [
    "In order to estimate the fractional component of the offset, we will fit a 2nd-order polynomial function to a few points of the correlation signal near the peak. From the coefficients of the polynomial, we can then estimate the fractional offset. In this simple example, we will fit a polynomial using 3 points of the correlation surface centered on the peak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd9a68-95b1-426d-8270-db9bf6f05f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabolic_interpolation(correlation, peak_index):\n",
    "    vertex = 0.5 * (correlation[peak_index - 1] - correlation[peak_index + 1]) / (correlation[peak_index - 1] - 2 * correlation[peak_index] + correlation[peak_index + 1])\n",
    "    return peak_index + vertex\n",
    "\n",
    "# Estimate the fractional offset using the parabolic fit\n",
    "frac_offset = parabolic_interpolation(normalized_corr, peak_index) - (len(x) - 1)\n",
    "print('Estimated offset:', frac_offset)\n",
    "print('True offset:', true_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826996f2-b39f-441e-9a74-6c12be0aec49",
   "metadata": {},
   "source": [
    "Data noise can impact the estimate of the offset by lowering the signal-to-noise ratio (SNR) of the correlation signal peak. Lower SNR will lead to less-reliable offset measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d62d03-b12e-4d4d-96a3-9ee55504276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = [0.1, 0.5, 1.0, 2.0]  # Different noise levels for SNR variation\n",
    "\n",
    "# Perform simulations\n",
    "errors = []\n",
    "for noise_level in noise_levels:\n",
    "    # Generate signals\n",
    "    x, y = generate_signals(true_shift, noise_level)\n",
    "    fig, axes = plt.subplot_mosaic('ab', figsize=(9, 4))\n",
    "    axes['a'].plot(x, alpha=0.4)\n",
    "    axes['a'].plot(y, alpha=0.4)\n",
    "    axes['a'].set_ylabel('Signal amplitude')\n",
    "\n",
    "    # Correlate\n",
    "    corr = correlate(y, x, mode='full')\n",
    "    peak_index = np.argmax(corr)\n",
    "    normalized_corr = corr / corr[peak_index]\n",
    "    c_index = np.arange(normalized_corr.size) - 1000\n",
    "    axes['b'].plot(c_index, normalized_corr)\n",
    "    axes['b'].set_ylim(-0.5, 1.05)\n",
    "    axes['b'].set_ylabel('SNR')\n",
    "    axes['b'].set_xlabel('Integer offset')\n",
    "\n",
    "    # Compute SNR and estimate shift\n",
    "    noise_index = np.setxor1d(peak_index, np.arange(x.size))\n",
    "    snr = corr[peak_index] / np.std(corr[noise_index])\n",
    "    estimated_shift = parabolic_interpolation(corr, peak_index) - (len(x) - 1)\n",
    "    fig.suptitle(f'SNR: {snr:4.1f}, estimated shift: {estimated_shift:4.2f}')\n",
    "    fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c1ffb-a64d-4148-adc7-6bbb1cbe5ad8",
   "metadata": {},
   "source": [
    "Additionally, the length of the signal (which is equivalent to the window length in our examples) can significantly impact the offset estimate. Shorter signal lengths lead to noisier correlation surfaces and more errors in the offset estimate. However, in practice, longer signal lengths are not always better. If there is significant non-affine warping of the signal `y` relative to `x`, then a longer signal length would be more contaminated by this warping and would lead to inaccurate offset estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33067bf-a051-4165-9cdf-a94efd79620f",
   "metadata": {},
   "source": [
    "### 2D example\n",
    "\n",
    "Now let's explore the cross-correlation procedure in 2D by extracting a sample patch from the reference and secondary SLCs. Note, in the code below, we extract patches from pre-computed, multi-looked amplitude images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af25668-3836-4785-8362-6765b0299852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 128x128 secondary (template patch)\n",
    "sec_patch = np.load(os.path.join(parent_dir, 'support_files', 'sec_patch.npy'))\n",
    "win_size = sec_patch.shape[0]\n",
    "\n",
    "# Load 128x128 reference patch, which also includes a 20 pixel buffer for the search window\n",
    "ref_patch = np.load(os.path.join(parent_dir, 'support_files', 'ref_patch.npy'))\n",
    "search_win = (ref_patch.shape[0] - win_size) // 2\n",
    "\n",
    "# Remove mean value from patches\n",
    "ref_patch -= np.mean(ref_patch)\n",
    "sec_patch -= np.mean(sec_patch)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "im1 = ax1.imshow(ref_patch, cmap='gray')\n",
    "im2 = ax2.imshow(sec_patch, cmap='gray')\n",
    "im2.set_clim(im1.get_clim())\n",
    "ax1.set_title('Reference patch')\n",
    "ax2.set_title('Secondary patch')\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bcf50-9c02-4fa4-bfe0-466367f4b9f2",
   "metadata": {},
   "source": [
    "To perform the cross-correlation operation, we will use the match template function implemented in the OpenCV package (https://docs.opencv.org/4.5.2/d4/dc6/tutorial_py_template_matching.html) which outputs a correlation surface between a reference and template patch. This function is used as the core algorithm for autoRIFT. ISCE implements a similar version with additional machinery specifically for SLC images with varying frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b7509-93ad-4549-9c6b-ba6268640095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "# Call OpenCV template matching to get a correlation surface\n",
    "correlation = cv.matchTemplate(ref_patch.astype(np.float32),\n",
    "                               sec_patch.astype(np.float32),\n",
    "                               cv.TM_CCOEFF)\n",
    "\n",
    "# Construct arrays corresponding to the offset values\n",
    "xoff = np.arange(-search_win, search_win, 1)\n",
    "yoff = np.arange(-search_win, search_win, 1)\n",
    "\n",
    "# Find the integer offset\n",
    "indmax = np.argmax(correlation)\n",
    "row_max, col_max = np.unravel_index(indmax, correlation.shape)\n",
    "\n",
    "# Get the corresponding shift\n",
    "max_xoffset = xoff[col_max]\n",
    "max_yoffset = yoff[row_max]\n",
    "print('Integer shifts:', max_xoffset, max_yoffset)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(correlation, extent=[xoff[0], xoff[-1], yoff[-1], yoff[0]])\n",
    "cbar = plt.colorbar(im, ax=ax, orientation='vertical')\n",
    "cbar.set_label('Correlation')\n",
    "ax.set_xlabel('X shift (pixels)');\n",
    "ax.set_ylabel('Y shift (pixels)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e59af-2e35-4bae-9c5a-31759447953b",
   "metadata": {},
   "source": [
    "From the correlation surface, we can see a fairly concentrated peak at a coordinate of $\\Delta X$ = 0 pixels and $\\Delta Y$ = 3 pixels, which indicates most of the shift is in the azimuth direction. Now, we will extract a small sub-window around the correlation peak and oversample it in order to estimate the fractional offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31527dad-8c51-424d-97c2-b609ce98b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the peak\n",
    "peak_index = np.argmax(correlation)\n",
    "peak_row, peak_col = np.unravel_index(peak_index, correlation.shape)\n",
    "coarse_dx = peak_col - search_win\n",
    "coarse_dy = peak_row - search_win\n",
    "print('Coarse integer X offset:', coarse_dx)\n",
    "print('Coarse integer Y offset:', coarse_dy)\n",
    "\n",
    "# Slice a small window\n",
    "zoom_window = 8\n",
    "row_slice = slice(peak_row - zoom_window // 2, peak_row + zoom_window // 2)\n",
    "col_slice = slice(peak_col - zoom_window // 2, peak_col + zoom_window // 2)\n",
    "correlation_subset = correlation[row_slice, col_slice]\n",
    "\n",
    "# Call zoom function\n",
    "zoom_factor = 16\n",
    "correlation_zoom = ndimage.zoom(correlation_subset, zoom=zoom_factor, prefilter=False, order=4)\n",
    "\n",
    "# Compute the grid corresponding to the oversampled coordinates CENTERED on peak\n",
    "# These are the normalized \"sub-pixel\" coordinates\n",
    "x = np.linspace(-zoom_window // 2, zoom_window // 2 - 1, zoom_factor * zoom_window)\n",
    "y = np.linspace(-zoom_window // 2, zoom_window // 2 - 1, zoom_factor * zoom_window) # same as x\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Get index of peak oversampled correlation\n",
    "peak_zoom_index = np.argmax(correlation_zoom)\n",
    "peak_zoom_row, peak_zoom_col = np.unravel_index(peak_zoom_index, correlation_zoom.shape)\n",
    "\n",
    "# Get the sub-pixel offsets\n",
    "dx = X[peak_zoom_row, peak_zoom_col] + coarse_dx\n",
    "dy = Y[peak_zoom_row, peak_zoom_col] + coarse_dy\n",
    "print('Sub-pixel X offset:', dx)\n",
    "print('Sub-pixel Y offset:', dy)\n",
    "\n",
    "# Construct the image extent of the sub-window\n",
    "half_zoom = zoom_window // 2\n",
    "extent = np.array([-half_zoom + coarse_dx, half_zoom + coarse_dx,\n",
    "                   half_zoom + coarse_dy, -half_zoom + coarse_dy]) - 0.5\n",
    "\n",
    "# View the oversampled window next to the original\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "im1 = ax1.imshow(correlation_subset, extent=extent)\n",
    "im2 = ax2.imshow(correlation_zoom, extent=extent)\n",
    "# Plot location of sub-pixel peak\n",
    "for ax in (ax1, ax2):\n",
    "    ax.axvline(dx, ls='--', color='C3')\n",
    "    ax.axhline(dy, ls='--', color='C3')\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Oversampled')\n",
    "fig.set_tight_layout(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb2622-9725-4ac4-a146-6ae5895e81f3",
   "metadata": {},
   "source": [
    "After oversampling of the correlation surface by a factor 16, we now estimate $\\Delta X$ = -0.20 pixels and $\\Delta Y$ = 2.64 pixels. Note that the oversampling factor indicates that we can measure the pixel offsets to 1/16th of a pixel. However, this does __not__ mean that the accuracy of the measurement is 1/16th of a pixel. Recalling our 1D example above, the SNR and the the curvature of the correlation surface dictates the overall __uncertainty__ in the offset measurement. When the surface has a sharp peak, the uncertainty is lower. When the surface is more diffuse (spread out), the uncertainty is high. The ISCE dense offset module provides a measurement of this uncertainty via a covariance file `dense_ampcor_cov.bil`, which we will not discuss in this tutorial. Instead, we will utilize measurements of the SNR to mask out offsets with low SNR values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35060f-7060-44a4-bbc0-038c404f26db",
   "metadata": {},
   "source": [
    "## Visualize offsets\n",
    "\n",
    "After the dense offset computation is completed, we can visualized the offsets by calling a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e0f57-5cea-4989-9aaa-16e22c46496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_offsets(results_dir='merged', xlim=(-141.05, -139.5), ylim=(59.9, 60.32), snr_thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ffceb-398c-418e-ad03-01ba87112981",
   "metadata": {},
   "source": [
    "## Generating offsets with autoRIFT\n",
    "\n",
    "Alternatively to the dense offset computation included in the TOPS processing, we can make use of the `autoRIFT` package (https://github.com/nasa-jpl/autoRIFT), which is a more flexible, standalone package for finding pixel displacements between co-registered images of any format (including optical data). The basic offset computation algorithm is the same as the TOPS dense offsets (2D cross-correlation of image patches), but `autoRIFT` employs the `OpenCV` package to perform the cross-correlation. Moreover, `autoRIFT` allows you to provide land and ocean masks to limit search distances, as well as reference displacement rate maps to set search distances larger in fast moving areas and smaller in slow moving areas (e.g., on and off a fast-flowing glacier). These customizations can significantly reduce computational time compared to the standard dense offsets.\n",
    "\n",
    "For Sentinel-1 radar data, `autoRIFT` is also packaged with the `Geogrid` module, which helps with transformations between radar and geographic coordinates. Both the `autoRIFT` and `Geogrid` modules are included in ISCE (e.g., `components.contrib.geo_autoRIFT.autoRIFT` and `components.contrib.geo_autoRIFT.geogrid`). For an example on using `autoRIFT` on optical images, see [this](https://github.com/parosen/Geo-SInC/blob/main/UNAVCO2022/4.3_Offset_stack_for_velocity_dynamics_with_autoRIFT/autorift.ipynb) notebook from Forrest Williams.\n",
    "\n",
    "#### Implementation details\n",
    "\n",
    "For this notebook, we will be calling functions included in the following Python files, which are included in the autoRIFT repository:\n",
    "- `testGeogrid_ISCE.py`\n",
    "- `testautoRIFT_ISCE.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df899434-f22a-407b-910b-2df4102250f4",
   "metadata": {},
   "source": [
    "### Running `Geogrid`\n",
    "\n",
    "`Geogrid` is designed to compute transformations between pixels in image coordinates (e.g., range and azimuth in radar images, row and column for optical images) to pixels in an output map projection system. The output projection system is controlled by various input files passed into `Geogrid`, which specify spatially-varying parameters like window sizes, search window sizes, reference velocities, topography, and surface slopes __in map coordinates__.\n",
    "\n",
    "For this tutorial, we will use a cropped version of the DEM used for TOPS processing which has also been warped to the UTM North projection (`cropped_nasadem_utm.tif`). We will then create the necessary input rasters for `Geogrid`. The function below implements the input raster generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800e39a-7cd1-47ba-ae0a-711b76b8415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_geogrid_inputs(demfile, srx=960, sry=960, csx=240, csy=240):\n",
    "    \"\"\"\n",
    "    Generate the necessary input rasters for Geogrid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    demfile: str\n",
    "        Name of input DEM.\n",
    "    srx: float, optional\n",
    "        Search window velocity (m/yr) in the map X-direction. Default: 960.\n",
    "    sry: float, optional\n",
    "        Search window velocity (m/yr) in the map Y-direction. Default: 960.\n",
    "    csx: float, optional\n",
    "        Window size (m) in the map X-direction. Default: 240.\n",
    "    csy: float, optional\n",
    "        Window size (m) in the map Y-direction. Default: 240.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Load DEM elevation array in order to get shapes and geoTransform\n",
    "    ds = gdal.Open(demfile, gdal.GA_ReadOnly)\n",
    "    b = ds.GetRasterBand(1)\n",
    "    h = b.ReadAsArray()\n",
    "    geoTransform = ds.GetGeoTransform()\n",
    "    shape = h.shape\n",
    "    ds = None\n",
    "\n",
    "    # Compute slopes\n",
    "    dx = geoTransform[1]\n",
    "    dy = geoTransform[-1]\n",
    "    dhdx = np.gradient(h, dx, axis=1)\n",
    "    dhdy = np.gradient(h, dy, axis=0)\n",
    "    \n",
    "    # DEM slopes\n",
    "    utils.write_gdal('dhdx.tif', dhdx, geoTransform, dtype=gdal.GDT_Float32)\n",
    "    utils.write_gdal('dhdy.tif', dhdy, geoTransform, dtype=gdal.GDT_Float32)\n",
    "    \n",
    "    # Write chip window size files\n",
    "    utils.write_gdal('xMinChipSize.tif', csx * np.ones(shape, dtype=np.uint16), geoTransform, dtype=gdal.GDT_UInt16)\n",
    "    utils.write_gdal('yMinChipSize.tif', csy * np.ones(shape, dtype=np.uint16), geoTransform, dtype=gdal.GDT_UInt16)\n",
    "    utils.write_gdal('xMaxChipSize.tif', 4 * csx * np.ones(shape, dtype=np.uint16), geoTransform, dtype=gdal.GDT_UInt16)\n",
    "    utils.write_gdal('yMaxChipSize.tif', 4 * csy * np.ones(shape, dtype=np.uint16), geoTransform, dtype=gdal.GDT_UInt16)\n",
    "    \n",
    "    # Write search ranges\n",
    "    utils.write_gdal('vxSearchRange.tif', srx * np.ones(shape, dtype=np.int16), geoTransform, dtype=gdal.GDT_Int16)\n",
    "    utils.write_gdal('vySearchRange.tif', sry * np.ones(shape, dtype=np.int16), geoTransform, dtype=gdal.GDT_Int16)\n",
    "    \n",
    "    # Write stable surface mask\n",
    "    utils.write_gdal('StableSurface.tif', np.zeros(shape, dtype=np.uint8), geoTransform, dtype=gdal.GDT_Byte)\n",
    "    \n",
    "    # Write reference velocity field\n",
    "    utils.write_gdal('vx.tif', np.zeros(shape, dtype=np.float32), geoTransform, dtype=gdal.GDT_Float32)\n",
    "    utils.write_gdal('vy.tif', np.zeros(shape, dtype=np.float32), geoTransform, dtype=gdal.GDT_Float32)\n",
    "    \n",
    "os.chdir(parent_dir)\n",
    "demfile = 'cropped_nasadem_utm.tif'\n",
    "prepare_geogrid_inputs(demfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e3898-ef3d-4bc7-af73-d8fc7759972b",
   "metadata": {},
   "source": [
    "Once those files have been generated, we can run `Geogrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74453a5-831a-4bb9-8f6e-ea8ce92bbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the pair we are processing\n",
    "geogrid_info = {\n",
    "    'dem':  demfile,\n",
    "    'ssm': 'StableSurface.tif',\n",
    "    'dhdx': 'dhdx.tif',\n",
    "    'dhdy': 'dhdy.tif',\n",
    "    'srx': 'vxSearchRange.tif',\n",
    "    'sry': 'vySearchRange.tif',\n",
    "    'csminx': 'xMinChipSize.tif',\n",
    "    'csminy': 'yMinChipSize.tif',\n",
    "    'csmaxx': 'xMaxChipSize.tif',\n",
    "    'csmaxy': 'yMaxChipSize.tif',\n",
    "    'vx': 'vx.tif',\n",
    "    'vy': 'vy.tif'\n",
    "}\n",
    "\n",
    "# Load scene\n",
    "meta_r = loadMetadata('reference')\n",
    "meta_s = loadMetadata('secondary')\n",
    "\n",
    "# Run Geogrid\n",
    "geogrid_run_info = runGeogrid(\n",
    "    meta_r,\n",
    "    meta_s,\n",
    "    epsg=32607,\n",
    "    **geogrid_info\n",
    ")\n",
    "# Need to re-register all drivers\n",
    "gdal.AllRegister()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b9993-08fc-4722-973c-3c6800e28a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older versions of Geogrid (as used in this lab) do not create the file window_scale_factor.tif\n",
    "# used in autoRIFT. We'll generate it manually here\n",
    "ds = gdal.Open('window_offset.tif', gdal.GA_ReadOnly)\n",
    "geoTransform = ds.GetGeoTransform()\n",
    "ds = None\n",
    "out = np.ones((geogrid_run_info['ycount'], geogrid_run_info['xcount']), dtype=np.float64)\n",
    "utils.write_gdal('window_scale_factor.tif', [out, out], geoTransform, dtype=gdal.GDT_Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977197a-fb2e-4248-9e87-6359e978df82",
   "metadata": {},
   "source": [
    "The `Geogrid` outputs are the varioius `window_*tif` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53f77d-4b90-4382-afc7-d809aee6a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l window_*.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1656b-a08e-4872-900d-8febddd7787e",
   "metadata": {},
   "source": [
    "### Run `autoRIFT`\n",
    "\n",
    "To run `autoRIFT`, we simply need to pass in the outputs from `Geogrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde72cf-fb46-4cfd-9748-4e69d646aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary of inputs to autoRIFT from the outputs of Geogrid\n",
    "autorift_info = {\n",
    "    'grid_location': 'window_location.tif',\n",
    "    'init_offset': 'window_offset.tif',\n",
    "    'search_range': 'window_search_range.tif',\n",
    "    'chip_size_min': 'window_chip_size_min.tif',\n",
    "    'chip_size_max': 'window_chip_size_max.tif',\n",
    "    'stable_surface_mask': 'window_stable_surface_mask.tif',\n",
    "    'offset2vx': 'window_rdr_off2vel_x_vec.tif',\n",
    "    'offset2vy': 'window_rdr_off2vel_y_vec.tif',\n",
    "    'scale_factor': 'window_scale_factor.tif',\n",
    "    'mpflag': 8,\n",
    "    'preprocessing_filter_width': 21,\n",
    "}\n",
    "\n",
    "reference_path = 'merged/reference.slc.full'\n",
    "secondary_path = 'merged/secondary.slc.full'\n",
    "\n",
    "# Clean any old autoRIFT runs\n",
    "try:\n",
    "    os.remove('autoRIFT_intermediate.nc')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Run autoRIFT\n",
    "generateAutoriftProduct(\n",
    "    reference_path,\n",
    "    secondary_path,\n",
    "    nc_sensor=None,\n",
    "    optical_flag=False,\n",
    "    ncname=None,\n",
    "    geogrid_run_info=geogrid_run_info,\n",
    "    **autorift_info,\n",
    "    parameter_file=DEFAULT_PARAMETER_FILE.replace('/vsicurl/', ''),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588abe23-fd8e-4257-909f-74936a12fc5d",
   "metadata": {},
   "source": [
    "The `autoRIFT` outputs are the files `offset.tif` and `velocity.tif`. The first one, `offset.tif`, has four bands in the UTM North projection. The first band is the range offsets, the second band is the azimuth offsets, the third band is an interpolation mask, and the fourth band is the window size used in the range direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f708f7-24eb-4fb7-a856-01929c1d48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_autorift_offsets(filename='offset.tif', vx_clim=(-6, 6), vy_clim=(-6, 6),\n",
    "                            xtitle='Pixel offset in range direction',\n",
    "                            ytitle='Pixel offset in azimuth direction')\n",
    "\n",
    "# Let's also re-plot the ISCE dense offsets for comparision\n",
    "utils.plot_offsets(results_dir='merged', xlim=(-141.05, -139.5), ylim=(59.9, 60.32), snr_thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e153a-d092-4c54-81f3-68ac84995d70",
   "metadata": {},
   "source": [
    "The second file, `velocity.tif`, contains the same offset information as `offset.tif`, but the offsets have been transformed to velocities by: 1) including the time separation between the reference and secondary images; and 2) projecting the offsets in the range and azimuth directions to East-West by assuming the flow is parallel to the surface slope as computed by the DEM. This is a common procedure for glacier and landscape studies when offset information from a single look direction is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477f76a-7d31-4a70-a63c-68a4907ccaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_autorift_offsets(filename='velocity.tif', vx_clim=(-1500, 1500), vy_clim=(-2000, 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc5e65-adab-44d9-9702-b9dd7b502384",
   "metadata": {},
   "source": [
    "Overall, we can see that the offset field generated by `autoRIFT` is fairly similar to the one generated by the ISCE/TOPS dense offsets module. `autoRIFT` tends to be more aggressive in masking out bad values, but the offsets in the high SNR areas look to be very similar between the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc385ed-586d-4a98-950a-6aa0d1505c61",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-danger\">\n",
    "<font size=\"4\"> <b> <font color='rgba(200,0,0,0.2)'> <u>ASSIGNMENT</u>:  </font> Modify the dense offset parameters and re-run </b> </font>\n",
    "\n",
    "Modify the window sizes for the `topsApp` dense offsets module by adjusting the window widths/heights and the search window widths/heights in the `topsApp.xml` file and re-running `topsApp.py --start=denseoffsets --end=geocodeoffsets`. For example, try reducing the windows by a factor of two. How does this affect the quality of the offset fields? How does this affect the runtime of the dense offset computation?</i></b>.\n",
    "\n",
    "For plotting, you can place the files `filt_dense_offsets.bil.geo` and `dense_offsets_snr.bil.geo` for each new experiment in a new directory and pass that directory as `results_dir` to the function `utils.plot_offsets`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7eb7e-4019-4aae-a9dc-3be11f7d136d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthscope_insar [conda env:.local-earthscope_insar]",
   "language": "python",
   "name": "conda-env-.local-earthscope_insar-earthscope_insar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
